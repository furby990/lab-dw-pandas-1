{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25d7736c-ba17-4aff-b6bb-66eba20fbf4e",
   "metadata": {},
   "source": [
    "# Lab | Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1973e9e-8be6-4039-b70e-d73ee0d94c99",
   "metadata": {},
   "source": [
    "In this lab, we will be working with the customer data from an insurance company, which can be found in the CSV file located at the following link: https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv\n",
    "\n",
    "The data includes information such as customer ID, state, gender, education, income, and other variables that can be used to perform various analyses.\n",
    "\n",
    "Throughout the lab, we will be using the pandas library in Python to manipulate and analyze the data. Pandas is a powerful library that provides various data manipulation and analysis tools, including the ability to load and manipulate data from a variety of sources, including CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045146f-f4f7-44d9-8cd9-130d6400c73a",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "\n",
    "- Customer - Customer ID\n",
    "\n",
    "- ST - State where customers live\n",
    "\n",
    "- Gender - Gender of the customer\n",
    "\n",
    "- Education - Background education of customers \n",
    "\n",
    "- Customer Lifetime Value - Customer lifetime value(CLV) is the total revenue the client will derive from their entire relationship with a customer. In other words, is the predicted or calculated value of a customer over their entire duration as a policyholder with the insurance company. It is an estimation of the net profit that the insurance company expects to generate from a customer throughout their relationship with the company. Customer Lifetime Value takes into account factors such as the duration of the customer's policy, premium payments, claim history, renewal likelihood, and potential additional services or products the customer may purchase. It helps insurers assess the long-term profitability and value associated with retaining a particular customer.\n",
    "\n",
    "- Income - Customers income\n",
    "\n",
    "- Monthly Premium Auto - Amount of money the customer pays on a monthly basis as a premium for their auto insurance coverage. It represents the recurring cost that the insured person must pay to maintain their insurance policy and receive coverage for potential damages, accidents, or other covered events related to their vehicle.\n",
    "\n",
    "- Number of Open Complaints - Number of complaints the customer opened\n",
    "\n",
    "- Policy Type - There are three type of policies in car insurance (Corporate Auto, Personal Auto, and Special Auto)\n",
    "\n",
    "- Vehicle Class - Type of vehicle classes that customers have Two-Door Car, Four-Door Car SUV, Luxury SUV, Sports Car, and Luxury Car\n",
    "\n",
    "- Total Claim Amount - the sum of all claims made by the customer. It represents the total monetary value of all approved claims for incidents such as accidents, theft, vandalism, or other covered events.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a72419b-20fc-4905-817a-8c83abc59de6",
   "metadata": {},
   "source": [
    "External Resources: https://towardsdatascience.com/filtering-data-frames-in-pandas-b570b1f834b9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8ece17-e919-4e23-96c0-c7c59778436a",
   "metadata": {},
   "source": [
    "## Challenge 1: Understanding the data\n",
    "\n",
    "In this challenge, you will use pandas to explore a given dataset. Your task is to gain a deep understanding of the data by analyzing its characteristics, dimensions, and statistical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91437bd5-59a6-49c0-8150-ef0e6e6eb253",
   "metadata": {},
   "source": [
    "- Identify the dimensions of the dataset by determining the number of rows and columns it contains.\n",
    "- Determine the data types of each column and evaluate whether they are appropriate for the nature of the variable. You should also provide suggestions for fixing any incorrect data types.\n",
    "- Identify the number of unique values for each column and determine which columns appear to be categorical. You should also describe the unique values of each categorical column and the range of values for numerical columns, and give your insights.\n",
    "- Compute summary statistics such as mean, median, mode, standard deviation, and quartiles to understand the central tendency and distribution of the data for numerical columns. You should also provide your conclusions based on these summary statistics.\n",
    "- Compute summary statistics for categorical columns and providing your conclusions based on these statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd4e8cd8-a6f6-486c-a5c4-1745b0c035f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Customer          ST GENDER             Education Customer Lifetime Value  \\\n",
      "0  RB50392  Washington    NaN                Master                     NaN   \n",
      "1  QZ44356     Arizona      F              Bachelor              697953.59%   \n",
      "2  AI49188      Nevada      F              Bachelor             1288743.17%   \n",
      "3  WW63253  California      M              Bachelor              764586.18%   \n",
      "4  GA49547  Washington      M  High School or Below              536307.65%   \n",
      "\n",
      "    Income  Monthly Premium Auto Number of Open Complaints     Policy Type  \\\n",
      "0      0.0                1000.0                    1/0/00   Personal Auto   \n",
      "1      0.0                  94.0                    1/0/00   Personal Auto   \n",
      "2  48767.0                 108.0                    1/0/00   Personal Auto   \n",
      "3      0.0                 106.0                    1/0/00  Corporate Auto   \n",
      "4  36357.0                  68.0                    1/0/00   Personal Auto   \n",
      "\n",
      "   Vehicle Class  Total Claim Amount  \n",
      "0  Four-Door Car            2.704934  \n",
      "1  Four-Door Car         1131.464935  \n",
      "2   Two-Door Car          566.472247  \n",
      "3            SUV          529.881344  \n",
      "4  Four-Door Car           17.269323  \n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the provided URL into a pandas DataFrame\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ef9cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 4008\n",
      "Number of columns: 11\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the DataFrame\n",
    "num_rows, num_columns = df.shape\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17ace8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types of each column:\n",
      " Customer                      object\n",
      "ST                            object\n",
      "GENDER                        object\n",
      "Education                     object\n",
      "Customer Lifetime Value       object\n",
      "Income                       float64\n",
      "Monthly Premium Auto         float64\n",
      "Number of Open Complaints     object\n",
      "Policy Type                   object\n",
      "Vehicle Class                 object\n",
      "Total Claim Amount           float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get the data types of each column\n",
    "data_types = df.dtypes\n",
    "print(\"Data types of each column:\\n\", data_types)\n",
    "\n",
    "# Suggest fixing for incorrect data types\n",
    "# For example, converting object columns that are numeric in nature:\n",
    "df['Customer Lifetime Value'] = pd.to_numeric(df['Customer Lifetime Value'].str.replace(',', ''), errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c03854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values per column:\n",
      " Customer                     1071\n",
      "ST                              8\n",
      "GENDER                          5\n",
      "Education                       6\n",
      "Customer Lifetime Value         0\n",
      "Income                        774\n",
      "Monthly Premium Auto          132\n",
      "Number of Open Complaints       6\n",
      "Policy Type                     3\n",
      "Vehicle Class                   6\n",
      "Total Claim Amount            761\n",
      "dtype: int64\n",
      "Categorical columns: ['Customer', 'ST', 'GENDER', 'Education', 'Number of Open Complaints', 'Policy Type', 'Vehicle Class']\n",
      "Unique values in Customer:\n",
      " ['RB50392' 'QZ44356' 'AI49188' ... 'CW49887' 'MY31220' nan]\n",
      "Unique values in ST:\n",
      " ['Washington' 'Arizona' 'Nevada' 'California' 'Oregon' 'Cali' 'AZ' 'WA'\n",
      " nan]\n",
      "Unique values in GENDER:\n",
      " [nan 'F' 'M' 'Femal' 'Male' 'female']\n",
      "Unique values in Education:\n",
      " ['Master' 'Bachelor' 'High School or Below' 'College' 'Bachelors' 'Doctor'\n",
      " nan]\n",
      "Unique values in Number of Open Complaints:\n",
      " ['1/0/00' '1/2/00' '1/1/00' '1/3/00' '1/5/00' '1/4/00' nan]\n",
      "Unique values in Policy Type:\n",
      " ['Personal Auto' 'Corporate Auto' 'Special Auto' nan]\n",
      "Unique values in Vehicle Class:\n",
      " ['Four-Door Car' 'Two-Door Car' 'SUV' 'Luxury SUV' 'Sports Car'\n",
      " 'Luxury Car' nan]\n"
     ]
    }
   ],
   "source": [
    "# Number of unique values for each column\n",
    "unique_values = df.nunique()\n",
    "print(\"Unique values per column:\\n\", unique_values)\n",
    "\n",
    "# Identify categorical columns (often object type or with few unique values)\n",
    "categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
    "print(\"Categorical columns:\", categorical_columns)\n",
    "\n",
    "# Describe unique values for categorical columns\n",
    "for col in categorical_columns:\n",
    "    print(f\"Unique values in {col}:\\n\", df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56fa059e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for numerical columns:\n",
      "        Customer Lifetime Value        Income  Monthly Premium Auto  \\\n",
      "count                      0.0   1071.000000           1071.000000   \n",
      "mean                       NaN  39295.701214            193.234360   \n",
      "std                        NaN  30469.427060           1601.190369   \n",
      "min                        NaN      0.000000             61.000000   \n",
      "25%                        NaN  14072.000000             68.000000   \n",
      "50%                        NaN  36234.000000             83.000000   \n",
      "75%                        NaN  64631.000000            109.500000   \n",
      "max                        NaN  99960.000000          35354.000000   \n",
      "\n",
      "       Total Claim Amount  \n",
      "count         1071.000000  \n",
      "mean           404.986909  \n",
      "std            293.027260  \n",
      "min              0.382107  \n",
      "25%            202.157702  \n",
      "50%            354.729129  \n",
      "75%            532.800000  \n",
      "max           2893.239678  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert [['RB50392' 'QZ44356' 'AI49188' ... nan nan nan]\n ['Washington' 'Arizona' 'Nevada' ... nan nan nan]\n [nan 'F' 'F' ... nan nan nan]\n ['Master' 'Bachelor' 'Bachelor' ... nan nan nan]] to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Additional statistics: mean, median, mode, std deviation\u001b[39;00m\n\u001b[32m      6\u001b[39m numerical_mode = df.mode().iloc[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# Mode\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m numerical_median = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m      \u001b[38;5;66;03m# Median\u001b[39;00m\n\u001b[32m      8\u001b[39m numerical_std = df.std()            \u001b[38;5;66;03m# Standard deviation\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMean values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m, numerical_summary.loc[\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11733\u001b[39m, in \u001b[36mDataFrame.median\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  11725\u001b[39m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m, ndim=\u001b[32m2\u001b[39m))\n\u001b[32m  11726\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmedian\u001b[39m(\n\u001b[32m  11727\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  11731\u001b[39m     **kwargs,\n\u001b[32m  11732\u001b[39m ):\n\u001b[32m> \u001b[39m\u001b[32m11733\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11734\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Series):\n\u001b[32m  11735\u001b[39m         result = result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12496\u001b[39m, in \u001b[36mNDFrame.median\u001b[39m\u001b[34m(self, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12489\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmedian\u001b[39m(\n\u001b[32m  12490\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m  12491\u001b[39m     axis: Axis | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m  12494\u001b[39m     **kwargs,\n\u001b[32m  12495\u001b[39m ) -> Series | \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m> \u001b[39m\u001b[32m12496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12497\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedian\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnanmedian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m  12498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:12442\u001b[39m, in \u001b[36mNDFrame._stat_function\u001b[39m\u001b[34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[39m\n\u001b[32m  12438\u001b[39m nv.validate_func(name, (), kwargs)\n\u001b[32m  12440\u001b[39m validate_bool_kwarg(skipna, \u001b[33m\"\u001b[39m\u001b[33mskipna\u001b[39m\u001b[33m\"\u001b[39m, none_allowed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m> \u001b[39m\u001b[32m12442\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  12443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnumeric_only\u001b[49m\n\u001b[32m  12444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11589\u001b[39m, in \u001b[36mDataFrame._reduce\u001b[39m\u001b[34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[39m\n\u001b[32m  11585\u001b[39m     df = df.T\n\u001b[32m  11587\u001b[39m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[32m  11588\u001b[39m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11589\u001b[39m res = \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblk_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11590\u001b[39m out = df._constructor_from_mgr(res, axes=res.axes).iloc[\u001b[32m0\u001b[39m]\n\u001b[32m  11591\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m out.dtype != \u001b[33m\"\u001b[39m\u001b[33mboolean\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1519\u001b[39m, in \u001b[36mBlockManager.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m   1517\u001b[39m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] = []\n\u001b[32m   1518\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks:\n\u001b[32m-> \u001b[39m\u001b[32m1519\u001b[39m     nbs = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m     res_blocks.extend(nbs)\n\u001b[32m   1522\u001b[39m index = Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:406\u001b[39m, in \u001b[36mBlock.reduce\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    400\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[32m    404\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m2\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.values.ndim == \u001b[32m1\u001b[39m:\n\u001b[32m    409\u001b[39m         res_values = result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:11508\u001b[39m, in \u001b[36mDataFrame._reduce.<locals>.blk_func\u001b[39m\u001b[34m(values, axis)\u001b[39m\n\u001b[32m  11506\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array([result])\n\u001b[32m  11507\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m> \u001b[39m\u001b[32m11508\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:147\u001b[39m, in \u001b[36mbottleneck_switch.__call__.<locals>.f\u001b[39m\u001b[34m(values, axis, skipna, **kwds)\u001b[39m\n\u001b[32m    145\u001b[39m         result = alt(values, axis=axis, skipna=skipna, **kwds)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     result = \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py:787\u001b[39m, in \u001b[36mnanmedian\u001b[39m\u001b[34m(values, axis, skipna, mask)\u001b[39m\n\u001b[32m    785\u001b[39m     inferred = lib.infer_dtype(values)\n\u001b[32m    786\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m inferred \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmixed\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalues\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to numeric\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    789\u001b[39m     values = values.astype(\u001b[33m\"\u001b[39m\u001b[33mf8\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Cannot convert [['RB50392' 'QZ44356' 'AI49188' ... nan nan nan]\n ['Washington' 'Arizona' 'Nevada' ... nan nan nan]\n [nan 'F' 'F' ... nan nan nan]\n ['Master' 'Bachelor' 'Bachelor' ... nan nan nan]] to numeric"
     ]
    }
   ],
   "source": [
    "# Summary statistics for numerical columns\n",
    "numerical_summary = df.describe()\n",
    "print(\"Summary statistics for numerical columns:\\n\", numerical_summary)\n",
    "\n",
    "# Additional statistics: mean, median, mode, std deviation\n",
    "numerical_mode = df.mode().iloc[0]  # Mode\n",
    "numerical_median = df.median()      # Median\n",
    "numerical_std = df.std()            # Standard deviation\n",
    "\n",
    "print(\"Mean values:\\n\", numerical_summary.loc['mean'])\n",
    "print(\"Median values:\\n\", numerical_median)\n",
    "print(\"Mode values:\\n\", numerical_mode)\n",
    "print(\"Standard deviation:\\n\", numerical_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ea1f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics for categorical columns:\n",
      "        Customer      ST GENDER Education Number of Open Complaints  \\\n",
      "count      1071    1071    954      1071                      1071   \n",
      "unique     1071       8      5         6                         6   \n",
      "top     RB50392  Oregon      F  Bachelor                    1/0/00   \n",
      "freq          1     320    457       324                       830   \n",
      "\n",
      "          Policy Type  Vehicle Class  \n",
      "count            1071           1071  \n",
      "unique              3              6  \n",
      "top     Personal Auto  Four-Door Car  \n",
      "freq              780            576  \n"
     ]
    }
   ],
   "source": [
    "# Describe categorical columns\n",
    "categorical_summary = df.describe(include=['object'])\n",
    "print(\"Summary statistics for categorical columns:\\n\", categorical_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a703890-63db-4944-b7ab-95a4f8185120",
   "metadata": {},
   "source": [
    "## Challenge 2: analyzing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776a403-c56a-452f-ac33-5fd4fdb06fc7",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedbc484-da4d-4f9c-9343-e1d44311a87e",
   "metadata": {},
   "source": [
    "The marketing team wants to know the top 5 less common customer locations. Create a pandas Series object that contains the customer locations and their frequencies, and then retrieve the top 5 less common locations in ascending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dca5073-4520-4f42-9390-4b92733284ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Less Common Customer Locations:\n",
      " ST\n",
      "AZ             25\n",
      "WA             30\n",
      "Washington     81\n",
      "Nevada         98\n",
      "Cali          120\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the URL into DataFrame (if not already loaded)\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Calculate the frequency of each location (assuming 'ST' is the column for locations)\n",
    "location_frequencies = df['ST'].value_counts()\n",
    "\n",
    "# Step 2: Identify the top 5 less common locations in ascending order\n",
    "less_common_locations = location_frequencies.nsmallest(5).sort_values(ascending=True)\n",
    "\n",
    "# Output the results\n",
    "print(\"Top 5 Less Common Customer Locations:\\n\", less_common_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce80f43-4afa-43c7-a78a-c917444da4e0",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "The sales team wants to know the total number of policies sold for each type of policy. Create a pandas Series object that contains the policy types and their total number of policies sold, and then retrieve the policy type with the highest number of policies sold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f13997-1555-4f98-aca6-970fda1d2c3f",
   "metadata": {},
   "source": [
    "*Hint:*\n",
    "- *Using value_counts() method simplifies this analysis.*\n",
    "- *Futhermore, there is a method that returns the index of the maximum value in a column or row.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcfad6c1-9af2-4b0b-9aa9-0dc5c17473c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of policies sold for each type:\n",
      " Policy Type\n",
      "Personal Auto     780\n",
      "Corporate Auto    234\n",
      "Special Auto       57\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Policy type with the highest number of policies sold: Personal Auto\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data from the URL into a DataFrame (if not already loaded)\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Calculate the total number of policies sold for each policy type\n",
    "policy_counts = df['Policy Type'].value_counts()\n",
    "\n",
    "# Step 2: Retrieve the policy type with the highest number of policies sold\n",
    "most_sold_policy_type = policy_counts.idxmax()\n",
    "\n",
    "# Output the results\n",
    "print(\"Total number of policies sold for each type:\\n\", policy_counts)\n",
    "print(f\"\\nPolicy type with the highest number of policies sold: {most_sold_policy_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b863fd3-bf91-4d5d-86eb-be29ed9f5b70",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "The sales team wants to know if customers with Personal Auto have a lower income than those with Corporate Auto. How does the average income compare between the two policy types?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1386d75-2810-4aa1-93e0-9485aa12d552",
   "metadata": {},
   "source": [
    "- Use *loc* to create two dataframes: one containing only Personal Auto policies and one containing only Corporate Auto policies.\n",
    "- Calculate the average income for each policy.\n",
    "- Print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c0563cf-6f8b-463d-a321-651a972f82e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average income for Personal Auto policies: $38180.70\n",
      "Average income for Corporate Auto policies: $41390.31\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from the URL into a DataFrame (if not already loaded)\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Use loc to filter the DataFrame for each policy type\n",
    "personal_auto_df = df.loc[df['Policy Type'] == 'Personal Auto']\n",
    "corporate_auto_df = df.loc[df['Policy Type'] == 'Corporate Auto']\n",
    "\n",
    "# Step 2: Calculate average income for each policy type\n",
    "avg_income_personal_auto = personal_auto_df['Income'].mean()\n",
    "avg_income_corporate_auto = corporate_auto_df['Income'].mean()\n",
    "\n",
    "# Step 3: Print the results\n",
    "print(f\"Average income for Personal Auto policies: ${avg_income_personal_auto:.2f}\")\n",
    "print(f\"Average income for Corporate Auto policies: ${avg_income_corporate_auto:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b16c27-f4a5-4727-a229-1f88671cf4e2",
   "metadata": {},
   "source": [
    "### Bonus: Exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac584986-299b-475f-ac2e-928c16c3f512",
   "metadata": {},
   "source": [
    "Your goal is to identify customers with a high policy claim amount.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- Review again the statistics for total claim amount to gain an understanding of the data.\n",
    "- To identify potential areas for improving customer retention and profitability, we want to focus on customers with a high policy claim amount. Consider customers with a high policy claim amount to be those in the top 25% of the total claim amount. Create a pandas DataFrame object that contains information about customers with a policy claim amount greater than the 75th percentile.\n",
    "- Use DataFrame methods to calculate summary statistics about the high policy claim amount data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3af5f1-6023-4b05-9c01-d05392daa650",
   "metadata": {},
   "source": [
    "*Note: When analyzing data, we often want to focus on certain groups of values to gain insights. Percentiles are a useful tool to help us define these groups. A percentile is a measure that tells us what percentage of values in a dataset are below a certain value. For example, the 75th percentile represents the value below which 75% of the data falls. Similarly, the 25th percentile represents the value below which 25% of the data falls. When we talk about the top 25%, we are referring to the values that fall above the 75th percentile, which represent the top quarter of the data. On the other hand, when we talk about the bottom 25%, we are referring to the values that fall below the 25th percentile, which represent the bottom quarter of the data. By focusing on these groups, we can identify patterns and trends that may be useful for making decisions and taking action.*\n",
    "\n",
    "*Hint: look for a method that gives you the percentile or quantile 0.75 and 0.25 for a Pandas Series.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d234634-50bd-41e0-88f7-d5ba684455d1",
   "metadata": {},
   "source": [
    "*Hint 2: check `Boolean selection according to the values of a single column` in https://towardsdatascience.com/filtering-data-frames-in-pandas-b570b1f834b9*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b731bca6-a760-4860-a27b-a33efa712ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Total Claim Amount:\n",
      " count    1071.000000\n",
      "mean      404.986909\n",
      "std       293.027260\n",
      "min         0.382107\n",
      "25%       202.157702\n",
      "50%       354.729129\n",
      "75%       532.800000\n",
      "max      2893.239678\n",
      "Name: Total Claim Amount, dtype: float64\n",
      "\n",
      "75th percentile of Total Claim Amount: 532.8\n",
      "\n",
      "Summary statistics for high policy claim amount customers:\n",
      "              Income  Monthly Premium Auto  Total Claim Amount\n",
      "count    264.000000            264.000000          264.000000\n",
      "mean   23677.344697            165.193182          782.228263\n",
      "std    27013.483721            623.930992          292.751640\n",
      "min        0.000000             63.000000          537.600000\n",
      "25%        0.000000             99.000000          606.521741\n",
      "50%    18807.000000            114.000000          679.597985\n",
      "75%    42423.750000            133.250000          851.400000\n",
      "max    99316.000000          10202.000000         2893.239678\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Your code here\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the URL into DataFrame (if not already loaded)\n",
    "url = 'https://raw.githubusercontent.com/data-bootcamp-v4/data/main/file1.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Review statistics for the Total Claim Amount\n",
    "claim_amount_stats = df['Total Claim Amount'].describe()\n",
    "print(\"Statistics for Total Claim Amount:\\n\", claim_amount_stats)\n",
    "\n",
    "# Step 2: Calculate the 75th percentile\n",
    "claim_amount_75th_percentile = df['Total Claim Amount'].quantile(0.75)\n",
    "print(f\"\\n75th percentile of Total Claim Amount: {claim_amount_75th_percentile}\")\n",
    "\n",
    "# Step 3: Filter DataFrame for customers with high policy claim amounts (greater than 75th percentile)\n",
    "high_claim_customers = df[df['Total Claim Amount'] > claim_amount_75th_percentile]\n",
    "\n",
    "# Step 4: Calculate and print summary statistics for this subset\n",
    "high_claims_summary = high_claim_customers.describe()\n",
    "print(\"\\nSummary statistics for high policy claim amount customers:\\n\", high_claims_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
